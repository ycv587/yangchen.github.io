<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-programming-flag.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-programming-flag.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yangchen.pro","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Regression 就是找到一个函数 function，通过输入特征 x，输出一个数值 Scalar。">
<meta property="og:type" content="article">
<meta property="og:title" content="2-回归（Regression）">
<meta property="og:url" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/index.html">
<meta property="og:site_name" content="CBlog">
<meta property="og:description" content="Regression 就是找到一个函数 function，通过输入特征 x，输出一个数值 Scalar。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/pokeman-parameters.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/model.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/goodness-of-function.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/loss-function.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/loss-figure.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/best-function.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/gradient-descent.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/gradient-two-parameters.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/gradient-stuck.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/results.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-2.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-3.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-4.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-5.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-compare.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-overfitting.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/hidden-factors.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/new-model.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/new-results.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/regularization.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/regularization-performance.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/L1L2regularization.png">
<meta property="og:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/L1-L2.png">
<meta property="article:published_time" content="2020-09-19T08:21:41.000Z">
<meta property="article:modified_time" content="2020-09-23T05:04:20.308Z">
<meta property="article:author" content="YANG CHEN">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/pokeman-parameters.png">

<link rel="canonical" href="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>2-回归（Regression） | CBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="CBlog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">CBlog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录科研日常</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">4</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/ycv587" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YANG CHEN">
      <meta itemprop="description" content="is me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          2-回归（Regression）
        </h1>

        <div class="post-meta">

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-09-19 16:21:41" itemprop="dateCreated datePublished" datetime="2020-09-19T16:21:41+08:00">2020-09-19</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-09-23 13:04:20" itemprop="dateModified" datetime="2020-09-23T13:04:20+08:00">2020-09-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/" itemprop="url" rel="index"><span itemprop="name">“有手就行”系列</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>15k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>14 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Regression 就是找到一个函数 function，通过输入特征 x，输出一个数值 Scalar。</p>
<a id="more"></a>
<h1 id="1-回归模型"><a href="#1-回归模型" class="headerlink" title="1. 回归模型"></a>1. 回归模型</h1><h2 id="1-1-回归定义"><a href="#1-1-回归定义" class="headerlink" title="1.1 回归定义"></a>1.1 回归定义</h2><p>Regression 就是找到一个函数 function ，通过输入特征 x，输出一个数值 Scalar。</p>
<h2 id="1-2-应用举例"><a href="#1-2-应用举例" class="headerlink" title="1.2 应用举例"></a>1.2 应用举例</h2><ul>
<li>股市预测（Stock market forecast）<ul>
<li>输入：过去10年股票的变动、新闻咨询、公司并购咨询等</li>
<li>输出：预测股市明天的平均值</li>
</ul>
</li>
<li>自动驾驶（Self-driving Car）<ul>
<li>输入：无人车上的各个sensor的数据，例如路况、测出的车距等</li>
<li>输出：方向盘的角度</li>
</ul>
</li>
<li>商品推荐（Recommendation）<ul>
<li>输入：商品A的特性，商品B的特性</li>
<li>输出：购买商品B的可能性</li>
</ul>
</li>
<li>Pokemon精灵攻击力预测（Combat Power of a pokemon）：<ul>
<li>输入：进化前的CP值、物种（Bulbasaur）、血量（HP）、重量（Weight）、高度（Height）</li>
<li>输出：进化后的CP值</li>
</ul>
</li>
</ul>
<h2 id="1-3-模型步骤"><a href="#1-3-模型步骤" class="headerlink" title="1.3 模型步骤"></a>1.3 模型步骤</h2><ul>
<li>step1：模型假设，选择模型框架（线性模型）</li>
<li>step2：模型评估，如何判断众多模型的好坏（损失函数）</li>
<li>step3：模型优化，如何筛选最优的模型（梯度下降）</li>
</ul>
<h1 id="2-以预测宝可梦的-CP-值为例"><a href="#2-以预测宝可梦的-CP-值为例" class="headerlink" title="2. 以预测宝可梦的 CP 值为例"></a>2. 以预测宝可梦的 CP 值为例</h1><p>我们期望根据已有的宝可梦进化前后的信息，来预测某只宝可梦进化后的cp值的大小</p>
<h2 id="2-1-确定-Senario、Task-和-Model"><a href="#2-1-确定-Senario、Task-和-Model" class="headerlink" title="2.1 确定 Senario、Task 和 Model"></a>2.1 确定 Senario、Task 和 Model</h2><p><strong>Senario</strong></p>
<p>首先根据已有的data来确定Senario，我们拥有宝可梦进化前后cp值的这样一笔数据，input是进化前的宝可梦(包括它的各种属性)，output是进化后的宝可梦的cp值；因此我们的data是labeled，使用的Senario是Supervised Learning</p>
<p><strong>Task</strong></p>
<p>然后根据我们想要function的输出类型来确定Task，我们预期得到的是宝可梦进化后的cp值，是一个scalar，因此使用的Task是Regression</p>
<p><strong>Model</strong></p>
<p>关于Model，选择很多，这里采用的是Non-linear Model</p>
<h2 id="2-2-设定具体参数"><a href="#2-2-设定具体参数" class="headerlink" title="2.2 设定具体参数"></a>2.2 设定具体参数</h2><p>$X$：表示一只宝可梦，用下标表示该宝可梦的某种属性<br>$X_{cp}$：表示该宝可梦进化前的cp值<br>$X_s$：表示该宝可梦是属于哪一种物种，比如妙瓜种子、皮卡丘…<br>$X_{hp}$：表示该宝可梦的hp值即生命值是多少<br>$X_w$：代表该宝可梦的重重量<br>$X_h$：代表该宝可梦的高度<br>$f()$：表示我们要找的 function<br>$y$：表示 function 的 output，即宝可梦进化后的 cp 值，是一个标量（scalar）</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/pokeman-parameters.png" alt="pokeman-parameters" style="width:60%;"></center>

<h2 id="2-3-具体过程"><a href="#2-3-具体过程" class="headerlink" title="2.3 具体过程"></a>2.3 具体过程</h2><p><strong>回顾一下 machine Learning 的三个步骤：</strong></p>
<ul>
<li>定义一个 model 即 function set</li>
<li>定义一个 goodness of function 损失函数去评估该 function 的好坏</li>
<li>找一个最好的 function</li>
</ul>
<h3 id="2-3-1-Step1：模型选择（Model）"><a href="#2-3-1-Step1：模型选择（Model）" class="headerlink" title="2.3.1 Step1：模型选择（Model）"></a>2.3.1 Step1：模型选择（Model）</h3><p>如何选择一个 function 的模型呢？毕竟只有确定了模型才能调参。这里没有明确的思路，只能凭经验去一种种地试</p>
<h4 id="Linear-Model-线性模型"><a href="#Linear-Model-线性模型" class="headerlink" title="Linear Model 线性模型"></a>Linear Model 线性模型</h4><script type="math/tex; mode=display">
y=b+w \cdot X_{cp}</script><p>$y$ 代表进化后的cp值，$X_{cp}$ 代表进化前的 cp 值，$w$ 和 $b$ 代表未知参数，可以是任何数值。</p>
<p>根据不同的 $w$ 和 $b$，可以确定不同的无穷无尽的 function，而 $y=b+w \cdot X_{cp}$ 这个抽象出来的式子就叫做 model，是以上这些具体化的 function 的集合，即 function set。</p>
<p>上述提到的实际上是一种<strong>Linear Model</strong>，但只考虑了宝可梦进化前的 cp 值，而宝可梦还有更多的特征，因此我们可以将其扩展为：</p>
<script type="math/tex; mode=display">
y=b+\sum w_ix_i</script><p>其中：<br><strong>$x_i$</strong>： an attribute of input X  ( $x_i$ 也被称作 feature，即特征值)<br><strong>$w_i$</strong>：$x_i$ 的权重<br><strong>$b$</strong>：bias（偏置）</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/model.png" alt="model" style="width:60%;"></center>

<h3 id="2-3-2-Step2：模型评估（-Goodness-of-function）"><a href="#2-3-2-Step2：模型评估（-Goodness-of-function）" class="headerlink" title="2.3.2 Step2：模型评估（ Goodness of function）"></a>2.3.2 Step2：模型评估（ Goodness of function）</h3><h4 id="1-参数说明"><a href="#1-参数说明" class="headerlink" title="1) 参数说明"></a>1) 参数说明</h4><p>$x^i$：用上标来表示一个完整的 object 的编号，$x^{i}$表示第 i 只宝可梦(下标表示该 object 中的 component)<br>$\widehat{y}^i$：用$\widehat{y}$表示一个实际观察到的object输出，上标为i表示是第i个object</p>
<p>注：由于regression的输出值是scalar，因此$\widehat{y}$里面并没有component，只是一个简单的数值；但是未来如果考虑structured Learning的时候，我们output的object可能是有structured的，所以我们还是会需要用上标下标来表示一个完整的output的object和它包含的component</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/goodness-of-function.png" alt="goodness-of-function" style="width:60%;"></center>

<h4 id="2-损失函数（Loss-function）"><a href="#2-损失函数（Loss-function）" class="headerlink" title="2) 损失函数（Loss function）"></a>2) 损失函数（Loss function）</h4><p>为了衡量function set中的某个function的好坏，我们需要一个评估函数，即 <strong>Loss function</strong>，损失函数，简称<code>L</code>；<code>loss function</code>是一个 function 的 function：</p>
<script type="math/tex; mode=display">
L(f)=L(w,b)</script><p>input：a function；<br>output：how bad/good it is</p>
<p>由于 $f:y=b+w \cdot x_{cp}$，即 $f$ 是由 $w$ 和 $b$ 决定的，因此输入 $f$ 就等价于输入这个 $f$ 里的 $w$ 和$b$ ，因此<strong>Loss function实际上是在衡量一组参数的好坏</strong>。</p>
<p>之前提到的model是由我们自主选择的，这里的 loss function 也是，最常用的方法就是采用类似于方差和的形式来衡量参数的好坏，即预测值与真值差的平方和；这里真正的数值减估测数值的平方，叫做估测误差（Estimation error），将10个估测误差合起来就是 loss function</p>
<script type="math/tex; mode=display">
L(f)=L(w,b)=\sum_{n=1}^{10}(\widehat{y}^n-(b+w \cdot {x}^n_{cp}))^2</script><p>如果 $L(f)$ 越大，说明该 function 表现得越不好；$L(f)$ 越小，说明该 function 表现得越好</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/loss-function.png" alt="loss-function" style="width:60%;"></center>

<p><strong>Loss function可视化</strong></p>
<p>下图中是loss function的可视化，该图中的每一个点都代表一组 $(w,b)$，也就是对应着一个<code>function</code>；而该点的颜色对应着的loss function的结果 $L(w,b)$，它表示该点对应 function 的表现有多糟糕，颜色越偏红色代表 Loss 的数值越大，这个 function 的表现越不好，越偏蓝色代表 Loss 的数值越小，这个 function 的表现越好。</p>
<p>比如图中用红色箭头标注的点就代表了 $b=-180 , w=-2$ 对应的 function，即 $y=-180-2 \cdot x_{cp}$，该点所在的颜色偏向于红色区域，因此这个 function 的 loss 比较大，表现并不好。</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/loss-figure.png" alt="loss-figure" style="width:60%;"></center>

<h3 id="2-3-3-Step3：最佳模型（Pick-the-Best-Function）"><a href="#2-3-3-Step3：最佳模型（Pick-the-Best-Function）" class="headerlink" title="2.3.3 Step3：最佳模型（Pick the Best Function）"></a>2.3.3 Step3：最佳模型（Pick the Best Function）</h3><p>我们已经确定了loss function，他可以衡量我们的model里面每一个function的好坏，接下来我们要做的事情就是，从这个function set里面，挑选一个最好的function。</p>
<p>挑选最好的function这一件事情，写成formulation/equation的样子如下：</p>
<script type="math/tex; mode=display">
f^*={arg} \underset{f}{min} L(f)</script><p>或者是</p>
<script type="math/tex; mode=display">
w^*,b^*={arg}\ \underset{w,b}{min} L(w,b)={arg}\  \underset{w,b}{min} \sum\limits^{10}_{n=1}(\widehat{y}^n-(b+w \cdot x^n_{cp}))^2</script><p>也就是那个使 $L(f)=L(w,b)=Loss$ 最小的 $f$ 或 $(w,b)$，就是我们要找的 $f^<em>$ 或 $(w^</em>,b^*)$（有点像极大似然估计的思想）</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/best-function.png" alt="best-function" style="width:60%;"></center>

<p>利用线性代数的知识，可以解得这个 closed-form solution，但这里采用的是一种更为普遍的方法：<strong>梯度下降法(Gradient descent)</strong></p>
<h1 id="3-梯度下降（Gradient-Descent）"><a href="#3-梯度下降（Gradient-Descent）" class="headerlink" title="3. 梯度下降（Gradient Descent）"></a>3. 梯度下降（Gradient Descent）</h1><p>上面的例子比较简单，用线性代数的知识就可以解；但是对于更普遍的问题来说，<strong>gradient descent 的厉害之处在于，只要 $L(f)$ 是可微分的，gradient descent都可以拿来处理这个 $f$，找到表现比较好的 parameters</strong></p>
<h2 id="3-1-单个参数的问题"><a href="#3-1-单个参数的问题" class="headerlink" title="3.1 单个参数的问题"></a>3.1 单个参数的问题</h2><p>以只带单个参数 $w$ 的Loss Function $L(w)$ 为例，首先保证 $L(w)$ 是<strong>可微</strong>的：</p>
<script type="math/tex; mode=display">
w^*={arg}\ \underset{w}{min} L(w)</script><p>我们的目标就是找到这个使 Loss 最小的 $w^*$，实际上就是寻找切线 L 斜率为 0 的 global minima 最小值点(注意，存在一些 local minima 极小值点，其斜率也是 0)</p>
<p>有一个暴力的方法是，穷举所有的 $w$ 值，去找到使 loss 最小的 $w^*$，但是这样做是没有效率的；而 gradient descent 就是用来解决这个效率问题的，梯度下降法的过程如下：</p>
<ul>
<li><p>首先随机选取一个初始的点 $w^0$ (当然也不一定要随机选取，如果有办法可以得到比较接近 $w^*$ 的表现得比较好的 $w^0$ 当初始点，可以有效地提高查找 $w^*$ 的效率)</p>
</li>
<li><p>计算 $L$ 在 $w=w^0$ 的位置的微分，即 $\frac{dL}{dw}|_{w=w^0}$，几何意义就是切线的斜率</p>
</li>
<li><p>如果切线斜率是负的（negative），那么就应该使 $w$ 变大，即往右踏一步；如果切线斜率是正的（positive），那么就应该使 $w$ 变小，即往左踏一步，每一步的步长 step size 就是  $w$ 的改变量</p>
<p>  $w$ 的改变量 step size 的大小取决于两件事：</p>
<ul>
<li><p>一是现在的微分值 $\frac{dL}{dw}$ 有多大，微分值越大代表现在在一个越陡峭的地方，那它要移动的距离就越大，反之就越小；</p>
</li>
<li><p>二是一个常数项 $η$，被称为<strong>学习率</strong>（learning rate），它决定了每次踏出的 step size 不只取决于现在的斜率，还取决于一个事先就定好的数值，如果 learning rate 比较大，那每踏出一步的时候，参数 $w$ 更新的幅度就比较大，反之参数更新的幅度就比较小<br>  如果 learning rate 设置的大一些，那机器学习的速度就会比较快；但是 learning rate 如果太大，可能就会跳过最合适的 global minima 的点</p>
</li>
</ul>
</li>
<li><p>因此每次参数更新的大小是 $η \frac{dL}{dw}$，为了满足斜率为负时 $w$ 变大，斜率为正时 $w$ 变小，应当使原来的 $w$ 减去更新的数值，即</p>
<script type="math/tex; mode=display">
w^1=w^0-η \frac{dL}{dw}|_{w=w^0} \\
w^2=w^1-η \frac{dL}{dw}|_{w=w^1} \\
w^3=w^2-η \frac{dL}{dw}|_{w=w^2} \\
... \\
w^{i+1}=w^i-η \frac{dL}{dw}|_{w=w^i} \\
if\ \ (\frac{dL}{dw}|_{w=w^i}==0) \ \ then \ \ stop; \\</script></li>
<li><p>此时$w^i$对应的斜率为0，我们找到了一个极小值local minima，这就出现了一个问题，当微分为0的时候，参数就会一直卡在这个点上没有办法再更新了，因此通过gradient descent找出来的solution其实并不是最佳解global minima。但幸运的是，在linear regression上，是没有local minima的，因此可以使用这个方法</p>
</li>
</ul>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/gradient-descent.png" alt="gradient-descent" style="width:60%;"></center>

<h2 id="3-2-两个参数的问题"><a href="#3-2-两个参数的问题" class="headerlink" title="3.2 两个参数的问题"></a>3.2 两个参数的问题</h2><p>今天要解决的关于宝可梦的问题，是含有two parameters的问题，即$(w^<em>,b^</em>)=arg\ \underset{w,b} {min} L(w,b)$</p>
<p>当然，它本质上处理单个参数的问题是一样的</p>
<ul>
<li><p>首先，也是随机选取两个初始值，$w^0$和$b^0$</p>
</li>
<li><p>然后分别计算 $(w^0,b^0)$ 这个点上，$L$ 对 $w$ 和 $b$ 的偏微分，即 $\frac{\partial L}{\partial w}|_{w=w^0,b=b^0}$ 和 $\frac{\partial L}{\partial b}|_{w=w^0,b=b^0}$</p>
</li>
<li><p>更新参数，当迭代跳出时，$(w^i,b^i)$ 对应着极小值点</p>
<script type="math/tex; mode=display">
  w^1=w^0-η\frac{\partial L}{\partial w}|_{w=w^0,b=b^0} \ \ \ \ \ \ \ \  \ b^1=b^0-η\frac{\partial L}{\partial b}|_{w=w^0,b=b^0} \\
  w^2=w^1-η\frac{\partial L}{\partial w}|_{w=w^1,b=b^1} \ \ \ \ \ \ \ \  \ b^2=b^1-η\frac{\partial L}{\partial b}|_{w=w^1,b=b^1} \\
  ... \\
  w^{i+1}=w^{i}-η\frac{\partial L}{\partial w}|_{w=w^{i},b=b^{i}} \ \ \ \ \ \ \ \  \ b^{i+1}=b^{i}-η\frac{\partial L}{\partial b}|_{w=w^{i},b=b^{i}} \\
  if(\frac{\partial L}{\partial w}==0 \&\& \frac{\partial L}{\partial b}==0) \ \ \ then \ \ stop</script></li>
</ul>
<p>实际上，$L$ 的 gradient 就是微积分中的那个梯度的概念，即</p>
<script type="math/tex; mode=display">
\nabla L=
\begin{bmatrix}
\frac{\partial L}{\partial w} \\
\frac{\partial L}{\partial b}
\end{bmatrix}_{gradient}</script><p>可视化效果如下：(三维坐标显示在二维图像中，loss的值用颜色来表示)</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/gradient-two-parameters.png" alt="gradient-two-parameters" style="width: 60%;"></center>

<p>横坐标是b，纵坐标是w，颜色代表loss的值，越偏蓝色表示loss越小，越偏红色表示loss越大</p>
<p><strong>每次计算得到的梯度 gradient，即由 $\frac{\partial L}{\partial b}$ 和 $\frac{\partial L}{\partial w}$ 组成的 vector 向量，就是该等高线的法线方向(对应图中红色箭头的反方向)；而 $(-η\frac{\partial L}{\partial b},-η\frac{\partial L}{\partial w})$ 的作用就是让原先的 $(w^i,b^i)$ 朝着 gradient 的反方向即等高线法线方向前进，其中学习率 $η$ 的作用是每次更新的跨度(对应图中红色箭头的长度)；经过多次迭代，最终 gradient 达到极小值点</strong></p>
<p>注：这里两个方向的学习率 $η$ 必须保持一致，这样每次更新坐标的 step size 是等比例缩放的，保证坐标前进的方向始终和梯度下降的方向一致；否则坐标前进的方向将会发生偏移</p>
<h2 id="3-3-梯度下降的缺点"><a href="#3-3-梯度下降的缺点" class="headerlink" title="3.3 梯度下降的缺点"></a>3.3 梯度下降的缺点</h2><p>gradient descent 有一个令人担心的地方，也就是我之前一直提到的，它每次迭代完毕，寻找到的梯度为 0 的点必然是极小值点，local minima；却不一定是最小值点，global minima</p>
<p>这会造成一个问题是说，如果 loss function 长得比较坑坑洼洼(极小值点比较多)，而每次初始化 $w^0$ 的取值又是随机的，这会造成每次 gradient descent 停下来的位置都可能是不同的极小值点；而且当遇到梯度比较平缓 (gradient≈0) 的时候，gradient descent 也可能会效率低下甚至可能会卡住；也就是说通过这个方法得到的结果，是看人品的。</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/gradient-stuck.png" alt="gradient-stuck" style="width:60%;"></center>

<p>但是！在<strong>线性回归</strong>中，损失函数实际上是一个<strong>凸函数</strong>（convex），是没有局部最优解的，它只有一个 global minima，visualize 出来的图像就是从里到外一圈一圈包围起来的椭圆形的等高线(就像前面的等高线图)，因此随便选一个起始点，根据 gradient descent 最终找出来的，都会是同一组参数。</p>
<h1 id="4-计算pokemon的梯度"><a href="#4-计算pokemon的梯度" class="headerlink" title="4. 计算pokemon的梯度"></a>4. 计算pokemon的梯度</h1><h2 id="4-1-偏微分的计算"><a href="#4-1-偏微分的计算" class="headerlink" title="4.1 偏微分的计算"></a>4.1 偏微分的计算</h2><p>现在我们来求具体的 $L$ 对 $w$ 和 $b$ 的偏微分</p>
<script type="math/tex; mode=display">
L(w,b)=\sum\limits_{n=1}^{10}(\widehat{y}^n-(b+w\cdot x_{cp}^n))^2 \\
\frac{\partial L}{\partial w}=\sum\limits_{n=1}^{10}2(\widehat{y}^n-(b+w\cdot x_{cp}^n))(-x_{cp}^n) \\
\frac{\partial L}{\partial b}=\sum\limits_{n=1}^{10}2(\widehat{y}^n-(b+w\cdot x_{cp}^n))(-1)</script><p>根据 gradient descent，我们得到的 $y=b+w\cdot x_{cp}$ 中最好的参数是 $b=-188.4, w=2.7$</p>
<p>我们需要有一套评估系统来评价我们得到的最后这个 function 和实际值的误差 error 的大小；这里我们将 training data 里每一只宝可梦 $i$ 进化后的实际 cp 值与预测值之差的绝对值叫做 $e^i$，而这些误差之和 Average Error on Training Data 为 $\sum\limits_{i=1}^{10}e^i=31.9$</p>
<blockquote>
<p>What we really care about is the error on new data (testing data)</p>
</blockquote>
<p>当然我们真正关心的是 generalization 的case，也就是用这个 model 去估测新抓到的 pokemon，误差会有多少，这也就是所谓的 testing data 的误差；于是又抓了 10 只新的 pokemon，算出来的Average Error on Testing Data 为 $\sum\limits_{i=1}^{10}e^i=35.0$；可见 training data 里得到的误差一般是要比 testing data 要小，这也符合常识</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/results.png" alt="results" style="width:60%;"></center>

<h2 id="4-2-How-can-we-do-better"><a href="#4-2-How-can-we-do-better" class="headerlink" title="4.2 How can we do better?"></a>4.2 How can we do better?</h2><p>我们有没有办法做得更好呢？这时就需要我们重新去设计 model；如果仔细观察一下上图的 data，就会发现在原先的 cp 值比较大和比较小的地方，预测值是相当不准的</p>
<p>实际上，从结果来看，最终的function可能不是一条直线，可能是稍微更复杂一点的曲线</p>
<h3 id="考虑-x-cp-2-的model"><a href="#考虑-x-cp-2-的model" class="headerlink" title="考虑$(x_{cp})^2$的model"></a>考虑$(x_{cp})^2$的model</h3><center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-2.png" alt="Xcp-2" style="width:50%;"></center>

<h3 id="考虑-x-cp-3-的model"><a href="#考虑-x-cp-3-的model" class="headerlink" title="考虑$(x_{cp})^3$的model"></a>考虑$(x_{cp})^3$的model</h3><center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-3.png" alt="Xcp-3" style="width:50%;"></center>

<h3 id="考虑-x-cp-4-的model"><a href="#考虑-x-cp-4-的model" class="headerlink" title="考虑$(x_{cp})^4$的model"></a>考虑$(x_{cp})^4$的model</h3><center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-4.png" alt="Xcp-4" style="width:50%;"></center>

<h3 id="考虑-x-cp-5-的model"><a href="#考虑-x-cp-5-的model" class="headerlink" title="考虑$(x_{cp})^5$的model"></a>考虑$(x_{cp})^5$的model</h3><center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-5.png" alt="Xcp-5" style="width:50%;"></center>

<h3 id="5个model的对比"><a href="#5个model的对比" class="headerlink" title="5个model的对比"></a>5个model的对比</h3><p>这 5 个模型的 training data 的表现：随着 $(x_{cp})^i$ 的高次项的增加，对应的 average error 会不断地减小；实际上这件事情非常容易解释，实际上低次的式子是高次的式子的特殊情况（令高次项 $(x_{cp})^i$ 对应的 $w_i$ 为0，高次式就转化成低次式）</p>
<p>也就是说，在gradient descent可以找到best function的前提下（多次式为Non-linear model，存在local optimal局部最优解，gradient descent不一定能找到global minima），function所包含的项的次数越高，越复杂，error 在 training data上 的表现就会越来越小；但是，我们关心的不是 model 在 training data 上的 error 表现，而是 model 在 testing data 上的 error 表现</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-compare.png" alt="compare" style="width:60%;"></center>

<p>在 training data 上，model 越复杂，error 就会越低；但是在 testing data 上，model 复杂到一定程度之后，error 非但不会减小，反而会暴增，在该例中，从含有 $(X_{cp})^4$ 项的 model 开始往后的 model，testing data 上的 error 出现了大幅增长的现象，通常被称为<strong>过拟合</strong>（Overfitting）</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/Xcp-overfitting.png" alt="overfitting" style="width:60%;"></center>

<p>因此 model 不是越复杂越好，而是选择一个最适合的 model，在本例中，包含 $(X_{cp})^3$ 的式子是最适合的 model。</p>
<h2 id="4-3-进一步讨论其他参数"><a href="#4-3-进一步讨论其他参数" class="headerlink" title="4.3 进一步讨论其他参数"></a>4.3 进一步讨论其他参数</h2><h3 id="物种-x-s-的影响"><a href="#物种-x-s-的影响" class="headerlink" title="物种 $x_s$ 的影响"></a>物种 $x_s$ 的影响</h3><p>之前我们的model只考虑了宝可梦进化前的 cp 值，这显然是不对的，除了 cp 值外，还受到物种 $x_s$ 的影响</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/hidden-factors.png" alt="hidden-factors" style="width:60%;"></center>

<p>因此我们重新设计model：</p>
<script type="math/tex; mode=display">
if \ \ x_s=Pidgey: \ \ \ \ \ \ \ y=b_1+w_1\cdot x_{cp} \\
if \ \ x_s=Weedle: \ \ \ \ \ \ y=b_2+w_2\cdot x_{cp} \\
if \ \ x_s=Caterpie: \ \ \ \ y=b_3+w_3\cdot x_{cp} \\
if \ \ x_s=Eevee: \ \ \ \ \ \ \ \ \ y=b_4+w_4\cdot x_{cp}</script><p>也就是根据不同的物种，设计不同的 linear model（这里 $x_s=species \ of \ x$），那如何将上面的四个 if 语句合并成一个 linear model 呢？</p>
<p>这里引入 $δ(条件表达式)$ 的概念，当条件表达式为 true，则 δ 为 1；当条件表达式为 false，则 δ 为 0，因此可以通过下图的方式，将 4 个 if 语句转化成同一个 linear model</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/new-model.png" alt="new-model" style="width:60%;"></center>

<p>有了上面这个 model 以后，我们分别得到了在 training data 和 testing data 上测试的结果：</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/new-results.png" alt="new-results" style="width:60%;"></center>

<h3 id="Hp值-x-hp-、height-值-x-h-、weight-值-x-w-的影响"><a href="#Hp值-x-hp-、height-值-x-h-、weight-值-x-w-的影响" class="headerlink" title="Hp值 $x_{hp}$、height 值 $x_h$、weight 值 $x_w$ 的影响"></a>Hp值 $x_{hp}$、height 值 $x_h$、weight 值 $x_w$ 的影响</h3><p>考虑所有可能有影响的参数，设计出这个最复杂的 model：</p>
<script type="math/tex; mode=display">
if \ \ x_s=Pidgey: \ \  \ \ y'=b_1+w_1\cdot x_{cp}+w_5\cdot(x_{cp})^2 \\
if \ \ x_s=Weedle: \ \ \ y'=b_2+w_2\cdot x_{cp}+w_6\cdot(x_{cp})^2 \\
if \ \ x_s=Pidgey: \ \ \ y'=b_3+w_3\cdot x_{cp}+w_7\cdot(x_{cp})^2 \\
if \ \ x_s=Eevee: \ \ \ \ y'=b_4+w_4\cdot x_{cp}+w_8\cdot(x_{cp})^2 \\
y=y'+w_9\cdot x_{hp}+w_{10}\cdot(x_{hp})^2+w_{11}\cdot x_h+w_{12}\cdot (x_h)^2+w_{13}\cdot x_w+w_{14}\cdot (x_w)^2</script><p>算出的 training error=1.9，但是，testing error=102.3！<strong>这么复杂的 model 很大概率会发生overfitting</strong>(按照我的理解，overfitting实际上是我们多使用了一些input的变量或是变量的高次项使曲线跟training data拟合的更好，但不幸的是这些项并不是实际情况下被使用的，于是这个model在testing data上会表现得很糟糕)，overfitting就相当于是那个范围更大的韦恩图，它包含了更多的函数更大的范围，代价就是在准确度上表现得更糟糕</p>
<h2 id="4-4-正则化解决过拟合-Regularization"><a href="#4-4-正则化解决过拟合-Regularization" class="headerlink" title="4.4 正则化解决过拟合(Regularization)"></a>4.4 正则化解决过拟合(Regularization)</h2><blockquote>
<p>Regularization 可以使曲线变得更加平滑（smooth），training data 上的 error 变大，但是  testing data 上的 error 变小。有关 Regularization 的具体原理说明详见下一部分</p>
</blockquote>
<p>原来的 loss function 只考虑了 prediction 的 error，即 $\sum\limits_i^n(\widehat{y}^i-(b+\sum\limits_{j}w_jx_j))^2$；而 regularization 则是在原来的 loss function 的基础上加上了一项 $\lambda\sum(w_i)^2$，就是把这个 model 里面所有的 $w_i$ 的平方和用 λ 加权(其中 i 代表遍历 n 个training data，j 代表遍历 model 的每一项)</p>
<p>也就是说，<strong>我们期待参数 $w_i$ 越小甚至接近于 0 的 function，为什么呢？</strong></p>
<p>因为参数值接近 0 的 function，是比较平滑的；所谓的平滑的意思是，当今天的输入有变化的时候，output 对输入的变化是比较不敏感的</p>
<p>举例来说，对 $y=b+\sum w_ix_i$ 这个 model，当 input 变化 $\Delta x_i$，output 的变化就是 $w_i\Delta x_i$，也就是说，如果 $w_i$ 越小越接近 0 的话，输出对输入就越不敏感（sensitive），我们的 function 就是一个越平滑的 unction；说到这里你会发现，我们之前没有把bias——b这个参数考虑进去的原因是<strong> bias 的大小跟 function 的平滑程度是没有关系的</strong>， bias 值的大小只是把 function 上下移动而已</p>
<p><strong>那为什么我们喜欢比较平滑的 function 呢？</strong></p>
<p>如果我们有一个比较平滑的function，由于输出对输入是不敏感的，测试的时候，一些noises噪声对这个平滑的function的影响就会比较小，而给我们一个比较好的结果</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/regularization.png" alt="regularization" style="width:60%;"></center>

<p><strong>注：这里的λ需要我们手动去调整以取得最好的值</strong></p>
<p>λ值越大代表考虑smooth的那个regularization那一项的影响力越大，我们找到的function就越平滑</p>
<p>观察下图可知，当我们的λ越大的时候，在training data上得到的error其实是越大的，但是这件事情是非常合理的，因为当λ越大的时候，我们就越倾向于考虑w的值而越少考虑error的大小；但是有趣的是，虽然在training data上得到的error越大，但是在testing data上得到的error可能会是比较小的</p>
<p>下图中，当λ从0到100变大的时候，training error不断变大，testing error反而不断变小；但是当λ太大的时候(&gt;100)，在testing data上的error就会越来越大</p>
<p><strong>我们喜欢比较平滑的function，因为它对noise不那么sensitive；但是我们又不喜欢太平滑的function，因为它就失去了对data拟合的能力；而function的平滑程度，就需要通过调整λ来决定</strong>，就像下图中，当λ=100时，在testing data上的error最小，因此我们选择λ=100</p>
<p>注：这里的 error 指的是 $\frac{1}{n}\sum\limits_{i=1}^n|\widehat{y}^i-y^i|$</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/regularization-performance.png" alt="regularization-performance" style="width:60%;"></center>

<h1 id="5-conclusion总结"><a href="#5-conclusion总结" class="headerlink" title="5. conclusion总结"></a>5. conclusion总结</h1><h2 id="关于-pokemon-的-cp-值预测的流程："><a href="#关于-pokemon-的-cp-值预测的流程：" class="headerlink" title="关于 pokemon 的 cp 值预测的流程："></a>关于 pokemon 的 cp 值预测的流程：</h2><ul>
<li><p>根据已有的data特点(labeled data，包含宝可梦及进化后的cp值)，确定使用supervised learning监督学习</p>
</li>
<li><p>根据output的特点(输出的是scalar数值)，确定使用regression回归(linear or non-linear)</p>
</li>
<li><p>考虑包括进化前cp值、species、hp等各方面变量属性以及高次项的影响，我们的model可以采用这些input的一次项和二次型之和的形式，如：</p>
<script type="math/tex; mode=display">
if \ \ x_s=Pidgey: \ \  \ \ y'=b_1+w_1\cdot x_{cp}+w_5\cdot(x_{cp})^2 \\
if \ \ x_s=Weedle: \ \ \ y'=b_2+w_2\cdot x_{cp}+w_6\cdot(x_{cp})^2 \\
if \ \ x_s=Pidgey: \ \ \ y'=b_3+w_3\cdot x_{cp}+w_7\cdot(x_{cp})^2 \\
if \ \ x_s=Eevee: \ \ \ \ y'=b_4+w_4\cdot x_{cp}+w_8\cdot(x_{cp})^2 \\
y=y'+w_9\cdot x_{hp}+w_{10}\cdot(x_{hp})^2+w_{11}\cdot x_h+w_{12}\cdot (x_h)^2+w_{13}\cdot x_w+w_{14}\cdot (x_w)^2</script></li>
<li><p>而为了保证function的平滑性，loss function应使用regularization，即$L=\sum\limits_{i=1}^n(\widehat{y}^i-y^i)^2+\lambda\sum\limits_{j}(w_j)^2$，注意bias——参数b对function平滑性无影响，因此不额外再次计入loss function(y的表达式里已包含w、b)</p>
</li>
<li><p>利用gradient descent对regularization版本的loss function进行梯度下降迭代处理，每次迭代都减去L对该参数的微分与learning rate之积，假设所有参数合成一个vector：$[w_0,w_1,w_2,…,w_j,…,b]^T$，那么每次梯度下降的表达式如下：</p>
<script type="math/tex; mode=display">
梯度:
\nabla L=
\begin{bmatrix}
\frac{\partial L}{\partial w_0} \\
\frac{\partial L}{\partial w_1} \\
\frac{\partial L}{\partial w_2} \\
... \\
\frac{\partial L}{\partial w_j} \\
... \\
\frac{\partial L}{\partial b}
\end{bmatrix}_{gradient}
\ \ \ 
gradient \ descent:
\begin{bmatrix}
w'_0\\
w'_1\\
w'_2\\
...\\
w'_j\\
...\\
b'
\end{bmatrix}_{L=L'}
= \ \ \ \ \ \ 
\begin{bmatrix}
w_0\\
w_1\\
w_2\\
...\\
w_j\\
...\\
b
\end{bmatrix}_{L=L_0}
-\ \ \ \ \eta
\begin{bmatrix}
\frac{\partial L}{\partial w_0} \\
\frac{\partial L}{\partial w_1} \\
\frac{\partial L}{\partial w_2} \\
... \\
\frac{\partial L}{\partial w_j} \\
... \\
\frac{\partial L}{\partial b}
\end{bmatrix}_{L=L_0}</script></li>
<li><p>当梯度稳定不变时，即$\nabla L$为0时，gradient descent便停止，此时如果采用的model是linear的，那么vector必然落于global minima处(凸函数)；如果采用的model是Non-linear的，vector可能会落于local minima处(此时需要采取其他办法获取最佳的function)</p>
<p>  假定我们已经通过各种方法到达了global minima的地方，此时的vector：$[w_0,w_1,w_2,…,w_j,…,b]^T$所确定的那个唯一的function就是在该λ下的最佳$f^*$，即loss最小</p>
</li>
<li><p>这里λ的最佳数值是需要通过我们不断调整来获取的，因此令λ等于0，10，100，1000，…不断使用gradient descent或其他算法得到最佳的parameters：$[w_0,w_1,w_2,…,w_j,…,b]^T$，并计算出这组参数确定的function——$f^*$对training data和testing data上的error值，直到找到那个使testing data的error最小的λ，(这里一开始λ=0，就是没有使用regularization时的loss function)</p>
<p>  注：引入评价$f^<em>$的error机制，令error=$\frac{1}{n}\sum\limits_{i=1}^n|\widehat{y}^i-y^i|$，分别计算该$f^</em>$对training data和testing data(more important)的$error(f^*)$大小</p>
<blockquote>
<p>先设定λ-&gt;确定loss function-&gt;找到使loss最小的$[w_0,w_1,w_2,…,w_j,…,b]^T$-&gt;确定function-&gt;计算error-&gt;重新设定新的λ重复上述步骤-&gt;使testing data上的error最小的λ所对应的$[w_0,w_1,w_2,…,w_j,…,b]^T$所对应的function就是我们能够找到的最佳的function</p>
</blockquote>
</li>
</ul>
<h2 id="本章总结："><a href="#本章总结：" class="headerlink" title="本章总结："></a>本章总结：</h2><ul>
<li><p>Pokémon: Original CP and species almost decide the CP after evolution </p>
</li>
<li><p>There are probably other hidden factors</p>
</li>
<li><p>Gradient descent</p>
</li>
<li><p>More theory and tips in the following lectures </p>
</li>
<li><p>Overfitting and Regularization</p>
</li>
<li><p>We finally get average error = 11.1 on the testing data</p>
</li>
<li><p>How about new data? Larger error? Lower error?(larger-&gt;need validation)</p>
</li>
<li><p>Next lecture: Where does the error come from?</p>
<ul>
<li>More theory about overfitting and regularization</li>
<li>The concept of validation(用来解决new data的error高于11.1的问题)</li>
</ul>
</li>
</ul>
<h1 id="附：L1-L2-正则化解决过拟合"><a href="#附：L1-L2-正则化解决过拟合" class="headerlink" title="附：L1 L2 正则化解决过拟合"></a>附：L1 L2 正则化解决过拟合</h1><p>关于overfitting的问题，很大程度上是由于曲线为了更好地拟合training data的数据，而引入了更多的高次项，使得曲线更加“蜿蜒曲折”，反而导致了对testing data的误差更大</p>
<p>回过头来思考，我们之前衡量model中某个function的好坏所使用的loss function，仅引入了真实值和预测值差值的平方和这一个衡量标准；我们想要避免overfitting过拟合的问题，就要使得高次项对曲线形状的影响尽可能小，因此我们要在loss function里引入高次项(非线性部分)的衡量标准，也就是将高次项的系数也加权放进loss function中，这样可以使得训练出来的model既满足预测值和真实值的误差小，又满足高次项的系数尽可能小而使曲线的形状比较稳定集中</p>
<p>以下图为例，如果loss function仅考虑了$(\widehat{y}-y)^2$这一误差衡量标准，那么拟合出来的曲线就是红色虚线部分(过拟合)，而过拟合就是所谓的model对training data过度自信, 非常完美的拟合上了这些数据, 如果具备过拟合的能力, 那么这个方程就可能是一个比较复杂的非线性方程 , 正是因为这里的$x^3$和$x^2$使得这条虚线能够被弯来弯去, 所以整个模型就会特别努力地去学习作用在$x^3$和$x^2$上的c、d参数. <strong>但是在这个例子里，我们期望模型要学到的却是这条蓝色的曲线. 因为它能更有效地概括数据</strong>.而且只需要一个$y=a+bx$就能表达出数据的规律. </p>
<p>或者是说, 蓝色的线最开始时, 和红色线同样也有c、d两个参数, 可是最终学出来时, c 和 d 都学成了0, 虽然蓝色方程的误差要比红色大, 但是概括起数据来还是蓝色好</p>
<center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/L1L2regularization.png" alt="regularization"></center>

<p>这也是我们通常采用的方法，我们不可能一开始就否定高次项而直接只采用低次线性表达式的model，因为有时候真实数据的确是符合高次项非线性曲线的分布的；而如果一开始直接采用高次非线性表达式的model，就很有可能造成overfitting，在曲线偏折的地方与真实数据的误差非常大。我们的目标应该是这样的：</p>
<p><strong>在无法确定真实数据分布的情况下，我们尽可能去改变loss function的评价标准</strong></p>
<ul>
<li><strong>我们的model的表达式要尽可能的复杂，包含尽可能多的参数和尽可能多的高次非线性项；</strong></li>
<li><strong>但是我们的loss function又有能力去控制这条曲线的参数和形状，使之不会出现overfitting过拟合的现象；</strong></li>
<li><strong>在真实数据满足高次非线性曲线分布的时候，loss function控制训练出来的高次项的系数比较大，使得到的曲线比较弯折起伏；</strong></li>
<li><strong>在真实数据满足低次线性分布的时候，loss function控制训练出来的高次项的系数比较小甚至等于0，使得到的曲线接近linear分布</strong></li>
</ul>
<p>那我们如何保证能学出来这样的参数呢? 这就是 L1 L2 正规化出现的原因.</p>
<p>之前的 loss function 仅考虑了 $(\widehat{y}-y)^2$ 这一误差衡量标准，而<strong>L1 L2正规化</strong>就是在这个loss function的后面多加了一个东西，即model中跟高次项系数有关的表达式；</p>
<ul>
<li><p>L1正规化即加上$λ\sum |w_j|$这一项，loss function变成$L=\sum\limits_{i=1}^n(\widehat{y}^i-y^i)^2+\lambda\sum\limits_{j}|w_j|$，即n个training data里的数据的真实值与预测值差值的平方和加上λ权重下的model表达式中所有项系数的绝对值之和</p>
</li>
<li><p>L2正规化即加上$\lambda\sum(w_j)^2$这一项，loss function变成$L=\sum\limits_{i=1}^n(\widehat{y}^i-y^i)^2+\lambda\sum\limits_{j}(w_j)^2$，即n个training data里的数据的真实值与预测值差值的平方和加上λ权重下的model表达式中所有项系数的平方和</p>
</li>
</ul>
<p>相对来说，L2要更稳定一些，L1的结果则不那么稳定，如果用p表示正规化程度，上面两式可总结如下：</p>
<script type="math/tex; mode=display">
L=\sum\limits_{i=1}^n(\widehat{y}^i-y^i)^2+\lambda\sum\limits_{j}(w_j)^p</script><center><img src="/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/L1-L2.png" alt="L1-L2"></center>
    </div>

    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>
        <div class="reward-container">
  <div>您的支持是对我最大的鼓励！</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="YANG CHEN 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="YANG CHEN 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>YANG CHEN
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yangchen.pro/2020/09/19/2-%E5%9B%9E%E5%BD%92%EF%BC%88Regression%EF%BC%89/" title="2-回归（Regression）">https://yangchen.pro/2020/09/19/2-回归（Regression）/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/09/19/1-%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="prev" title="1-什么是机器学习">
      <i class="fa fa-chevron-left"></i> 1-什么是机器学习
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/09/23/%E5%8A%A0%E5%AF%86%E6%B5%81%E9%87%8F%E8%AF%86%E5%88%AB%E8%AF%84%E4%BB%B7%E5%B8%B8%E7%94%A8%E6%8C%87%E6%A0%87/" rel="next" title="加密流量识别评价常用指标">
      加密流量识别评价常用指标 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#1-%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="nav-text">1. 回归模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-1-%E5%9B%9E%E5%BD%92%E5%AE%9A%E4%B9%89"><span class="nav-text">1.1 回归定义</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-2-%E5%BA%94%E7%94%A8%E4%B8%BE%E4%BE%8B"><span class="nav-text">1.2 应用举例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-3-%E6%A8%A1%E5%9E%8B%E6%AD%A5%E9%AA%A4"><span class="nav-text">1.3 模型步骤</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#2-%E4%BB%A5%E9%A2%84%E6%B5%8B%E5%AE%9D%E5%8F%AF%E6%A2%A6%E7%9A%84-CP-%E5%80%BC%E4%B8%BA%E4%BE%8B"><span class="nav-text">2. 以预测宝可梦的 CP 值为例</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#2-1-%E7%A1%AE%E5%AE%9A-Senario%E3%80%81Task-%E5%92%8C-Model"><span class="nav-text">2.1 确定 Senario、Task 和 Model</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-2-%E8%AE%BE%E5%AE%9A%E5%85%B7%E4%BD%93%E5%8F%82%E6%95%B0"><span class="nav-text">2.2 设定具体参数</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-3-%E5%85%B7%E4%BD%93%E8%BF%87%E7%A8%8B"><span class="nav-text">2.3 具体过程</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-1-Step1%EF%BC%9A%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A9%EF%BC%88Model%EF%BC%89"><span class="nav-text">2.3.1 Step1：模型选择（Model）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Linear-Model-%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="nav-text">Linear Model 线性模型</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-2-Step2%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%EF%BC%88-Goodness-of-function%EF%BC%89"><span class="nav-text">2.3.2 Step2：模型评估（ Goodness of function）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%8F%82%E6%95%B0%E8%AF%B4%E6%98%8E"><span class="nav-text">1) 参数说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%EF%BC%88Loss-function%EF%BC%89"><span class="nav-text">2) 损失函数（Loss function）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-3-Step3%EF%BC%9A%E6%9C%80%E4%BD%B3%E6%A8%A1%E5%9E%8B%EF%BC%88Pick-the-Best-Function%EF%BC%89"><span class="nav-text">2.3.3 Step3：最佳模型（Pick the Best Function）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%EF%BC%88Gradient-Descent%EF%BC%89"><span class="nav-text">3. 梯度下降（Gradient Descent）</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#3-1-%E5%8D%95%E4%B8%AA%E5%8F%82%E6%95%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-text">3.1 单个参数的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-2-%E4%B8%A4%E4%B8%AA%E5%8F%82%E6%95%B0%E7%9A%84%E9%97%AE%E9%A2%98"><span class="nav-text">3.2 两个参数的问题</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-3-%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E7%9A%84%E7%BC%BA%E7%82%B9"><span class="nav-text">3.3 梯度下降的缺点</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#4-%E8%AE%A1%E7%AE%97pokemon%E7%9A%84%E6%A2%AF%E5%BA%A6"><span class="nav-text">4. 计算pokemon的梯度</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#4-1-%E5%81%8F%E5%BE%AE%E5%88%86%E7%9A%84%E8%AE%A1%E7%AE%97"><span class="nav-text">4.1 偏微分的计算</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-2-How-can-we-do-better"><span class="nav-text">4.2 How can we do better?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%80%83%E8%99%91-x-cp-2-%E7%9A%84model"><span class="nav-text">考虑$(x_{cp})^2$的model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%80%83%E8%99%91-x-cp-3-%E7%9A%84model"><span class="nav-text">考虑$(x_{cp})^3$的model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%80%83%E8%99%91-x-cp-4-%E7%9A%84model"><span class="nav-text">考虑$(x_{cp})^4$的model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%80%83%E8%99%91-x-cp-5-%E7%9A%84model"><span class="nav-text">考虑$(x_{cp})^5$的model</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#5%E4%B8%AAmodel%E7%9A%84%E5%AF%B9%E6%AF%94"><span class="nav-text">5个model的对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-3-%E8%BF%9B%E4%B8%80%E6%AD%A5%E8%AE%A8%E8%AE%BA%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B0"><span class="nav-text">4.3 进一步讨论其他参数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%89%A9%E7%A7%8D-x-s-%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-text">物种 $x_s$ 的影响</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hp%E5%80%BC-x-hp-%E3%80%81height-%E5%80%BC-x-h-%E3%80%81weight-%E5%80%BC-x-w-%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-text">Hp值 $x_{hp}$、height 值 $x_h$、weight 值 $x_w$ 的影响</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-4-%E6%AD%A3%E5%88%99%E5%8C%96%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88-Regularization"><span class="nav-text">4.4 正则化解决过拟合(Regularization)</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#5-conclusion%E6%80%BB%E7%BB%93"><span class="nav-text">5. conclusion总结</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B3%E4%BA%8E-pokemon-%E7%9A%84-cp-%E5%80%BC%E9%A2%84%E6%B5%8B%E7%9A%84%E6%B5%81%E7%A8%8B%EF%BC%9A"><span class="nav-text">关于 pokemon 的 cp 值预测的流程：</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AC%E7%AB%A0%E6%80%BB%E7%BB%93%EF%BC%9A"><span class="nav-text">本章总结：</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%99%84%EF%BC%9AL1-L2-%E6%AD%A3%E5%88%99%E5%8C%96%E8%A7%A3%E5%86%B3%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-text">附：L1 L2 正则化解决过拟合</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YANG CHEN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YANG CHEN</p>
  <div class="site-description" itemprop="description">is me</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ycv587" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ycv587" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:not1st@163.com" title="E-Mail → mailto:not1st@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YANG CHEN</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">25k字</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">22 分钟</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    /*
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    */
    window.MathJax = {
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
