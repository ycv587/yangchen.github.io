<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.1.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-programming-flag.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-programming-flag.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yangchen.pro","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="1 回顾在上一篇文章中，我们讨论了如何通过样本点的均值 $u$ 和协方差 $\Sigma$ 来计算$P(C_1),P(C_2),P(x|C_1),P(x|C_2)$，进而计算得到新的样本点 $x$ 分别属于类别 1 和 2 的概率：  $P(C_1|x)&#x3D;\frac{P(C_1)P(x|C_1)}{P(C_1)P(x|C_1)+P(C_2)P(x|C_2)}$ $P(C_2|x)&#x3D;1-P(C_1|">
<meta property="og:type" content="article">
<meta property="og:title" content="7-逻辑回归（Logistic Regression）">
<meta property="og:url" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/index.html">
<meta property="og:site_name" content="CBlog">
<meta property="og:description" content="1 回顾在上一篇文章中，我们讨论了如何通过样本点的均值 $u$ 和协方差 $\Sigma$ 来计算$P(C_1),P(C_2),P(x|C_1),P(x|C_2)$，进而计算得到新的样本点 $x$ 分别属于类别 1 和 2 的概率：  $P(C_1|x)&#x3D;\frac{P(C_1)P(x|C_1)}{P(C_1)P(x|C_1)+P(C_2)P(x|C_2)}$ $P(C_2|x)&#x3D;1-P(C_1|">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/activation-function.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/likelihood.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/cross-entropy.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/sigmoid.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-contribute.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-simple.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-linear-regression.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-square.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/cross-entropy-vs-square-error.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/discriminative-generative.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/generative-discriminative-visualize.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/toy-example.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/multi-class.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/softmax.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-limitation.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/feature-transformation.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/cascade-logistic-regression.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-example.png">
<meta property="og:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/powerful-network.png">
<meta property="article:published_time" content="2020-10-04T15:03:58.000Z">
<meta property="article:modified_time" content="2020-10-08T11:40:59.921Z">
<meta property="article:author" content="YANG CHEN">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/activation-function.png">

<link rel="canonical" href="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>7-逻辑回归（Logistic Regression） | CBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="CBlog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">CBlog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录科研日常</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类<span class="badge">3</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档<span class="badge">9</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/ycv587" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="YANG CHEN">
      <meta itemprop="description" content="is me">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="CBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          7-逻辑回归（Logistic Regression）
        </h1>

        <div class="post-meta">

          
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-10-04 23:03:58" itemprop="dateCreated datePublished" datetime="2020-10-04T23:03:58+08:00">2020-10-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-10-08 19:40:59" itemprop="dateModified" datetime="2020-10-08T19:40:59+08:00">2020-10-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" itemprop="url" rel="index"><span itemprop="name">机器学习</span></a>
                </span>
                  ，
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/" itemprop="url" rel="index"><span itemprop="name">“有手就行”系列</span></a>
                </span>
            </span>

          
            <span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>8.6k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>8 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="1-回顾"><a href="#1-回顾" class="headerlink" title="1 回顾"></a>1 回顾</h4><p>在上一篇文章中，我们讨论了如何通过样本点的均值 $u$ 和协方差 $\Sigma$ 来计算$P(C_1),P(C_2),P(x|C_1),P(x|C_2)$，进而计算得到新的样本点 $x$ 分别属于类别 1 和 2 的概率：</p>
<ul>
<li>$P(C_1|x)=\frac{P(C_1)P(x|C_1)}{P(C_1)P(x|C_1)+P(C_2)P(x|C_2)}$</li>
<li>$P(C_2|x)=1-P(C_1|x)$</li>
</ul>
<p>…</p>
<a id="more"></a>
<p>之后我们还推导了 $P(C_1|x)=\sigma(z)=\frac{1}{1+e^{-z}}$，并且在高斯分布下考虑两个类别共用 $\Sigma$，可以得到一个线性的 $z$ (其实很多其他的概率模型经过化简以后也都可以得到同样的结果)</p>
<script type="math/tex; mode=display">
P_{w,b}(C_1|x)=\sigma(z)=\frac{1}{1+e^{-z}} \\
z=w\cdot x+b=\sum\limits_i w_ix_i+b</script><p>从上式中我们可以看出，现在这个模型是受 $w$ 和 $b$ 控制的，因此我们不必要再去像前面一样计算一大堆东西，而是用这个全新的由 $w$ 和 $b$ 决定的模型——<strong>逻辑回归(Logistic Regression)</strong></p>
<h4 id="2-逻辑回归"><a href="#2-逻辑回归" class="headerlink" title="2 逻辑回归"></a>2 逻辑回归</h4><h5 id="2-1-Step-1：function-set"><a href="#2-1-Step-1：function-set" class="headerlink" title="2.1 Step 1：function set"></a>2.1 Step 1：function set</h5><p>这里的模型就是Logistic Regression——逻辑回归</p>
<ul>
<li>$x_i$：输入</li>
<li>$w_i$：权重</li>
<li>$b$：偏置</li>
<li>$\sigma(z)$：sigmoid 函数</li>
</ul>
<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/activation-function.png" width="60%;"></center>

<h5 id="2-2-Step-2：Goodness-of-a-function"><a href="#2-2-Step-2：Goodness-of-a-function" class="headerlink" title="2.2 Step 2：Goodness of a function"></a>2.2 Step 2：Goodness of a function</h5><p>现在我们有 N 个训练数据，每一个数据都有标注它是属于哪一个类别</p>
<p>假设这些训练数据是从我们定义的后验概率产生的(某种意义上就是概率密度函数)，而 $w$ 和 $b$ 就决定了这个后验概率，那我们就可以去计算某一组 $w$ 和 $b$ 去产生这 N 个训练数据的概率，利用极大似然估计的思想，最好的那组参数就是有最大可能性产生当前 N 个训练数据分布的 $w^*$ 和 $b^*$</p>
<p>似然函数只需要将每一个点产生的概率相乘即可，注意，这里假定是二元分类，类别 2 的概率为 1 减去类别 1 的概率</p>
<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/likelihood.png" width="60%;"></center>

<p>由于 $L(w,b)$ 是乘积项的形式，为了方便计算，我们将上式做个变换：</p>
<script type="math/tex; mode=display">
\begin{split}
&w^*,b^*=\arg \max\limits_{w,b} L(w,b)=\arg\min\limits_{w,b}(-\ln L(w,b)) \\
&\begin{equation}
\begin{split}
-\ln L(w,b)=&-\ln f_{w,b}(x^1)\\
&-\ln f_{w,b}(x^2)\\
&-\ln(1-f_{w,b}(x^3))\\
&\ -...
\end{split}
\end{equation}
\end{split}</script><p>由于类别 1 和类别 2 的概率表达式不统一，上面的式子无法写成统一的形式，为了统一格式，这里将逻辑回归里的所有训练数据都打上 0 和 1 的标签，即输出 $\hat{y}=1$ 代表类别 1，输出 $\hat{y}=0$ 代表类别 2，于是上式进一步改写成：</p>
<script type="math/tex; mode=display">
\begin{split}
-\ln L(w,b)=&-[\hat{y}^1 \ln f_{w,b}(x^1)+(1-\hat{y}^1)ln(1-f_{w,b}(x^1))]\\
&-[\hat{y}^2 \ln f_{w,b}(x^2)+(1-\hat{y}^2)ln(1-f_{w,b}(x^2))]\\
&-[\hat{y}^3 \ln f_{w,b}(x^3)+(1-\hat{y}^3)ln(1-f_{w,b}(x^3))]\\
&\ -...
\end{split}</script><p>现在我们再整理一下格式：</p>
<script type="math/tex; mode=display">
-\ln L(w,b)=\sum\limits_n -[\hat{y}^n \ln f_{w,b}(x^n)+(1-\hat{y}^n) \ln(1-f_{w,b}(x^n))]</script><p>这里 $x^n$ 表示第 n 个样本点，$\hat{y}^n$ 表示第 n 个样本点的类别标签(1 表示类别 1,0表示类别 2)，里面其实是<u>两个伯努利分布（Bernoulli Distribution）的交叉熵（Cross Entropy）</u></p>
<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/cross-entropy.png" width="60%;"></center>

<p>假设有如上图所示的两个分布 $p$ 和 $q$，它们的交叉熵就是 $H(p,q)=-\sum\limits_{x} p(x) \ln (q(x))$，这也就是之前的推导中在 $-\ln L(w,b)$ 前加一个负号的原因</p>
<p>交叉熵的含义是表达这两个分布有多接近，如果 $p$ 和 $q$ 这两个分布一模一样的话，那它们算出来的交叉熵就是 0(详细解释在“信息论”中)，而这里 $f(x^n)$ 表示模型的输出值，$\hat{y}^n$ 表示标签值，因此<strong>交叉熵实际上表达的是希望这个模型的输出和它的标签（target）越接近越好</strong></p>
<p>总之，我们要找的参数实际上就是：</p>
<script type="math/tex; mode=display">
w^*,b^*=\arg \max\limits_{w,b} L(w,b)=\arg\min\limits_{w,b}(-\ln L(w,b)=\sum\limits_n -[\hat{y}^n \ln f_{w,b}(x^n)+(1-\hat{y}^n) \ln(1-f_{w,b}(x^n))]</script><h5 id="2-3-step-3：Find-the-best-function"><a href="#2-3-step-3：Find-the-best-function" class="headerlink" title="2.3 step 3：Find the best function"></a>2.3 step 3：Find the best function</h5><p>实际上就是去找到使损失函数即交叉熵之和最小的那组参数 $w^<em>,b^</em>$ 就行了，这里用梯度下降的方法进行运算就可以</p>
<p>这里 Sigmoid 函数的微分可以直接作为公式记下来：</p>
<script type="math/tex; mode=display">
\frac{\partial \sigma(z)}{\partial z}=\sigma(z)(1-\sigma(z))</script><p>Sigmoid和它的微分图像如下：</p>
<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/sigmoid.png" width="40%;"></center>

<p>先计算 $-\ln L(w,b)=\sum\limits_n -[\hat{y}^n \ln f_{w,b}(x^n)+(1-\hat{y}^n) \ln(1-f_{w,b}(x^n))]$ 对 $w_i$ 的偏微分，这里 $\hat{y}^n$ 和 $1-\hat{y}^n$ 是常数先不用管它，只需要分别求出 $\ln f_{w,b}(x^n)$ 和 $\ln (1-f_{w,b}(x^n))$ 对 $w_i$ 的偏微分即可，整体推导过程如下：</p>
<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-contribute.png" width="60%;"></center>

<p>将得到的式子进行进一步化简，可得：</p>
<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-simple.png" width="60%;"></center>

<p>我们发现最终的结果竟然异常的简洁，梯度下降每次更新只需要做：</p>
<script type="math/tex; mode=display">
w_i=w_i-\eta \sum\limits_{n}-(\hat{y}^n-f_{w,b}(x^n))x_i^n</script><p>那这个式子到底代表着什么意思呢？现在梯度更新只取决于三件事：</p>
<ul>
<li>$\eta$，学习率，是你自己设定的</li>
<li>$x_i$，来自于data</li>
<li>$\hat{y}^n-f_{w,b}(x^n)$，代表模型的输出跟标签值的差距有多大，如果离目标越远，更新的步伐就要越大</li>
</ul>
<h4 id="3-逻辑回归-vs-线性回归"><a href="#3-逻辑回归-vs-线性回归" class="headerlink" title="3 逻辑回归 vs 线性回归"></a>3 逻辑回归 vs 线性回归</h4><p>我们可以把逻辑回归和之前将的线性回归做一个比较</p>
<h5 id="3-1-模型"><a href="#3-1-模型" class="headerlink" title="3.1 模型"></a>3.1 模型</h5><p>逻辑回归是把每一个特征 $x_i$ 加权求和，加上偏置项，再通过 Sigmoid 函数得到输出</p>
<p>在函数的输出结果方面：因为逻辑回归的输出是通过 Sigmoid 函数产生的，因此一定是介于0~1之间；而线性回归的输出并没有通过 Sigmoid 函数，所以它可以是任何值</p>
<h5 id="3-2-损失函数"><a href="#3-2-损失函数" class="headerlink" title="3.2 损失函数"></a>3.2 损失函数</h5><p>在逻辑回归中，我们定义的损失函数，是所有样本点的输出值( $f(x^n)$ )和实际的标签值( $\hat{y}^n$ )在伯努利分布下的交叉熵总和</p>
<p><strong>交叉熵</strong>的描述：这里把$f(x^n)$和$\hat{y}^n$各自<u>看做</u>是一个<strong>Bernoulli distribution(两点分布)</strong>，那它们的交叉熵 $l(f(x^n),\hat{y}^n)=-[\hat{y}^n \ln f(x^n)+(1-\hat{y}^n) \ln (1-f(x^n))]$ 之和，就是我们要去minimize的对象，直观来讲，就是<strong>希望函数的输出值 $f(x^n)$ 和它的标签值 $\hat{y}^n$ 越接近越好</strong></p>
<p>注：这里的“看做”只是为了方便理解和计算，并不是真的做出它们是两点分布的假设</p>
<p>而在线性回归中，损失函数的定义相对比较简单，就是单纯的函数的输出值( $f(x^n)$ )和实际的标签值( $\hat{y}^n$ )在数值上的平方和的均值</p>
<p>这里可能会有一个疑惑，为什么逻辑回归的损失函数不能像线性回归一样用均方误差来表示呢？后面会有进一步的解释</p>
<h5 id="3-3-参数更新"><a href="#3-3-参数更新" class="headerlink" title="3.3 参数更新"></a>3.3 参数更新</h5><p>神奇的是，逻辑回归和线性回归的 $w_i$ 更新方式是一模一样的，唯一不同的是，逻辑回归的标签值 $\hat{y}^n$ 和输出值 $f(x^n)$ 都必须是在 0 和 1 之间的，而线性回归的标签和输出的范围可以是任意值</p>
<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-linear-regression.png" width="60%;"></center>
#### 4 为什么不使用均方误差？

之前提到了，为什么逻辑回归的损失函数不能用**均方误差**来描述呢？我们现在来试一下这件事情，重新进行机器学习

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-square.png" width="60%;"></center>
现在会遇到一个问题：如果第 n 个点属于类别 1，则 $\hat{y}^n=1$，此时如果函数的输出 $f_{w,b}(x^n)=1$ 的话，说明现在离目标很接近了，$f_{w,b}(x)-\hat{y}$ 这一项是0，于是得到的微分 $\frac{\partial L}{\partial w_i}$ 也是 0，这件事情是很合理的；但是当函数的输出 $f_{w,b}(x^n)=0$ 的时候，说明离目标还很遥远，但是由于在 *Step 3* 中求出来的更新表达式中有一个 $f_{w,b}(x^n)$，因此这个时候也会导致得到的微分 $\frac{\partial L}{\partial w_i}$ 变成 0

如果我们把参数的变化对**总损失**作图的话，损失函数选择交叉熵或均方差，参数的变化跟损失的变化情况可视化出来如下所示：(黑色的是交叉熵，红色的是均方误差)

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/cross-entropy-vs-square-error.png" width="60%;"></center>
假设中心点就是距离目标很近的地方，如果是交叉熵的话，距离目标越远，微分值就越大，参数更新的时候变化量就越大，迈出去的步伐也就越大

但当你选择均方误差的时候，过程就会很卡，因为距离目标远的时候，微分也是非常小的，移动的速度是非常慢的，我们之前提到过，实际操作的时候，当梯度接近于 0 的时候，其实就很有可能会停下来，因此使用均方误差很有可能在一开始的时候就卡住不动了，而且这里也不能随意地增大学习率，因为在做梯度下降的时候，你的梯度接近于 0，可能离目标很近也有可能很远，因此不知道学习率应该增大还是减小

综上，尽管均方误差可以使用，但是会出现更新十分缓慢的现象，而使用交叉熵损失函数可以让你的训练过程更顺利

#### 5 判别模型 vs 生成模型

逻辑回归方法，我们把它称之为判别（Discriminative）方法；而我们用高斯分布来描述后验概率这件事，我们称之为生成（Generative）方法

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/discriminative-generative.png" width="60%;"></center>
实际上它们用的模型是一模一样的，都是 $P(C_1|x)=\sigma(w\cdot x+b)$，如果是用逻辑回归的话，可以用梯度下降的方法直接去把 $w$ 和 $b$ 找出来；如果是用生成模型的话，我们要先去算 $u_1,u_2,\Sigma^{-1}$，然后算出 $w$ 和 $b$ 

你会发现用这两种方法得到的 $w$ 和 $b$ 是不同的，尽管我们的模型是同一个，但是由于做了不同的假设，最终从同样的训练集里找出来的参数会是不一样的

在逻辑回归里面，我们**没有做任何实质性的假设**，没有对后验概率有任何的描述，我们就是单纯地去找 $w$ 和 $b$ (推导过程中的假设只是便于理解和计算，对实际结果没有影响)

而在生成模型里面，我们对概率分布是**有实质性的假设**的，之前我们假设的是高斯分布，甚至假设在相互独立的前提下是否可以是朴素贝叶斯，根据这些假设我们才找到最终的 $w$ 和 $b$ 

哪一个假设的结果是比较好的呢？生成模型（Generative Model）和判别模型（Discriminative Model）的预测结果比较如下：

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/generative-discriminative-visualize.png" width="60%;"></center>
实际上判别方法常常会比生成方法表现得更好，这里举一个简单的例子来解释一下：



假设总共有两个类别，有这样的训练集：每个数据有两个特征，总共有 1+4+4+4=13 个数据

如果我们的测试集的两个特征都是1，凭直觉来说会认为它肯定属于类别 1，但是如果用朴素贝叶斯方法(假设所有的特征相互独立)，得到的结果又是怎样的呢？

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/toy-example.png" width="60%;"></center>
通过朴素贝叶斯方法得到的结果竟然是这个测试点属于类别 2 的可能性更大，这跟我们的直觉比起来是相反的，实际上我们直觉认为两个特征都是 1 的测试点属于类别 1 的可能性更大是因为我们潜意识里认为这两个特征之间是存在某种联系的，但是对朴素贝叶斯方法来说，它是不考虑不同维度之间的关系，朴素贝叶斯方法认为在维度间相互独立的前提下，类别 2 没有采样出两个特征都是 1 的数据，是因为采样的数量不够多，如果样本够多，它认为类别 2 观察到两个特征都是 1  的可能性会比类别 1 要大

在朴素贝叶斯方法中，类别 2 中找到样本点 $x$ 的概率是 $x$ 中第一个特征出现的概率与第二个特征出现的概率之积：$P(x|C_2)=P(x_1=1|C_2)\cdot P(x_2=1|C_2)$；但是我们的直觉告诉自己，两个特征之间肯定是有某种联系的，$P(x|C_2)$ 不能够那么轻易地被拆分成两个独立的概率乘积，也就是说朴素贝叶斯方法自作聪明地多假设了一些条件

所以，生成模型和判别的差别就在于，<span style="background-color:#fff000">生成模型它做了某些假设，假设你的数据来自于某个概率模型；而判别模型是完全不作任何假设的**</span>

生成模型做的事情就是脑补，它会自己去想象一些事情，于是会做出一个和我们人类直觉想法不太一样的判断结果，就像在这个例子中，我们使用朴素贝叶斯方法，于是朴素贝叶斯方法可能会在类别 2 里并没有出现过两个特征都是 1 的样本点的前提下，自己去脑补有这样的点

通常脑补不是一件好的事情，因为数据的各个维度之间很可能是有关联的，但是在数据很少的情况下，脑补也是有用的，判别模型并不是在所有的情况下都可以赢过生成模型，判别模型是十分依赖于数据的，当数据量不足或是数据本身的标签就有一些问题，那生成模型做一些脑补和假设，反而可以把数据的不足或是有问题部分的影响给降到最低

#### 6 总结

对于分类的问题(主要是二元分类)，我们一般有两种方法去处理问题，一种是生成方法，另一种是判别方法，注意到分类问题的模型都是从贝叶斯方程出发的，即
$$
\begin{split}
P(C_i|x)&=\frac{P(C_i)P(x|C_i)}{\sum\limits_{j=1}^nP(C_j)P(x|C_j)} \ \ (1) \\
&=\sigma(z)=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-(b+\sum\limits_k w_k x_k)}} \ \ (2)
\end{split}
$$
其中分子表示属于第 i 类的可能性，分母表示遍历从 1 到 n 所有的类的可能性，两种方法的区别在于：

生成模型会假设一个带参数的概率分布，利用这个假设的概率分布函数带入 (1) 式中去计算 $P(x|C_i)$ 和 $P(x|C_j)$，结合极大似然估计法最终得到最优的参数以确定这个模型的参数

判别模型不作任何假设，因此它无法通过假定的概率分布得到 $P(x|C_i)$ 的表达式，因此它使用的是 (2) 式，直接去利用交叉熵和梯度下降结合极大似然估计法得到最优的 $w$ 和 $b$，以确定模型

最后，利用得到的 $P(C_i|x)$ 与 0.5 相比较来判断它属于哪个类别的可能性更大

生成模型的好处是，它对数据的依赖并没有像判别模型那么严重，在数据较少或者数据本身就存在噪声的情况下受到的影响会更小。而判别模型的好处是，在数据充足的情况下，它训练出来的模型准确率一般比生成模型更好

#### 7 多分类问题

##### 7.1 Softmax

之前讲的都是二元分类的情况，这里讨论一下多元分类问题，其原理的推导过程与二元分类基本一致

假设有三个类别：$C_1,C_2,C_3$，每一个类别都有自己的权重 $w$ 和偏置 $b$，即 $w_1,w_2,w_3$ 和 $b_1,b_2,b_3$，输出 $x$ 和 $w_i$ 都是向量

> Softmax的意思是对最大值做强化，因为在做第一步的时候，对 $z$ 取指数会使大的值和小的值之间的差距被拉得更开，也就是强化大的值

我们把 $z_1,z_2,z_3$ 丢进一个 **Softmax** 函数，Softmax 做的事情是这样三步：

* 取指数，得到 $e^{z_1},e^{z_2},e^{z_3}$
* 把三个指数累计求和，得到总和 $\sum\limits_{j=1}^3 e^{z_j}$
* 将总和分别除去这三项(归一化)，得到 $y_1=\frac{e^{z_1}}{\sum\limits_{j=1}^3 e^{z_j}}$、$y_2=\frac{e^{z_2}}{\sum\limits_{j=1}^3 e^{z_j}}$ 和 $y_3=\frac{e^{z_3}}{\sum\limits_{j=1}^3 e^{z_j}}$

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/multi-class.png" width="60%;"></center>

<p>原来的输出 $z$ 可以是任何值，但是做完 Softmax 之后，输出 $y_i$ 的值一定是介于 0~1 之间，并且它们的和一定是 1，即 $\sum\limits_i y_i=1$，以上图为例，$y_i$ 表示输出的 $x$ 属于第 i 个类别的概率，比如属于 $C_1$ 的概率是 $y_1=0.88$，属于 $C_2$ 的概率是 $y_2=0.12$，属于 $C_3$ 的概率是 $y_3=0$</p>
<p>而 Softmax 的输出，就是拿来当 z 的后验概率</p>
<p>假设我们用的是高斯分布(共用协方差)，经过一般推导以后可以得到 Softmax 函数，而从信息论也可以推导出 Softmax 函数，<a href="https://en.wikipedia.org/wiki/Maximum_entropy">Maximum Entropy</a>本质内容和逻辑回归是一样的，它是从另一个观点来切入为什么我们的分类器长这个样子</p>
<h5 id="7-2-多分类过程"><a href="#7-2-多分类过程" class="headerlink" title="7.2 多分类过程"></a>7.2 多分类过程</h5><p>如下图所示，输入 $x$ 经过三个式子分别生成 $z_1,z_2,z_3$，经过 Softmax 转化成输出 $y_1,y_2,y_3$，我们不能使用 1,2,3 作为类别的区分，为了保证所有类别之间的关系是一样的，这里使用类似于one-hot编码的方式：</p>
<script type="math/tex; mode=display">
\hat{y}=
\begin{bmatrix}
1\\
0\\
0
\end{bmatrix}_{x \ ∈ \ class 1}
\hat{y}=
\begin{bmatrix}
0\\
1\\
0
\end{bmatrix}_{x \ ∈ \ class 2}
\hat{y}=
\begin{bmatrix}
0\\
0\\
1
\end{bmatrix}_{x \ ∈ \ class 3}</script><center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/softmax.png" width="60%;"></center>
这个时候就可以计算一下输出 $y$ 和  $\hat{y}$ 之间的交叉熵，即 $-\sum\limits_{i=1}^3 \hat{y}_i \ln y_i$，同二元分类一样，多元分类问题也是通过极大似然估计法得到最终的交叉熵表达式的，这里不再赘述

#####  7.3 逻辑回归的限制

Logistic Regression其实有很强的限制，给出下图的例子中的训练数据，想要用逻辑回归对它进行分类，其实是做不到的

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-limitation.png" width="60%;"></center>
因为逻辑回归在两个类别之间的分类面就是一条直线，但是在这个平面上无论怎么画直线都不可能把图中的两个类别分隔开来

如果坚持要用逻辑回归的话，有一招叫做**特征转换（Feature Transformation）**，原来的特征分布不好划分，那我们可以将之转化以后，找一个比较好的特征空间，让逻辑回归能够处理

假设这里定义 $x_1'$ 是原来的点到 $\begin{bmatrix}0\\0 \end{bmatrix}$ 之间的距离，$x_2'$ 是原来的点到 $\begin{bmatrix}1\\ 1 \end{bmatrix}$ 之间的距离，重新映射之后如下图右侧(红色两个点重合)，此时逻辑回归就可以把它们划分开来

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/feature-transformation.png" width="60%;"></center>
但麻烦的是，我们并不知道特征转换怎么做，如果在这上面花费太多的时间就得不偿失了，于是我们会希望这个转换是机器自己产生的，怎么让机器自己产生呢？<span style="background-color:#fff000">**我们可以让很多逻辑回归级联起来**</span>

我们让一个输入 $x$ 的两个特征 $x_1,x_2$ 经过两个逻辑回归的变换，得到新的特征 $x_1',x_2'$，在这个新的特征空间上，类别 1 和类别 2 是可以用一条直线分开的，那么最后只要再接另外一个逻辑回归的模型(对它来说，$x_1',x_2'$才是每一个样本点的"特征"，而不是原先的 $x_1,x_2$)，它根据新的特征，就可以把类别 1 和类别 2 分开

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/cascade-logistic-regression.png" width="60%;"></center>
因此着整个流程是，先用 n 个逻辑回归做特征转换(n为每个样本点的特征数量)，生成n个新的特征，然后再用一个逻辑回归作分类器

逻辑回归的分界线一定是一条直线，它可以有任何的画法，但肯定是按照某个方向从高到低的等高线分布，具体的分布是由逻辑回归的参数决定的，每一条直线都是由 $z=b+\sum\limits_i^nw_ix_i$ 组成的(二维特征的直线画在二维平面上，多维特征的直线则是画在多维空间上)

下图是二维特征的例子，分别表示四个点经过变换之后的 $x_1'$ 和 $x_2'$，在新的特征空间中可以通过最后的逻辑回归划分开来

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/logistic-example.png" width="60%;"></center>
注意，这里的逻辑回归只是一条直线，它指的是“属于这个类”或“不属于这个类”这两种情况，因此最后的这个逻辑回归是跟要检测的目标类相关的，当只是二元分类的时候，最后只需要一个逻辑回归即可，当面对多元分类问题，需要用到多个逻辑回归来画出多条直线划分所有的类，每一个逻辑回归对应它要检测的那个类



通过上面的例子，我们发现，多个逻辑回归连接起来会产生更强的效果，<span style="background-color:#fff000">**我们把每一个逻辑回归叫做一个神经元（Neuron），把这些逻辑回归串起来所形成的网络，就叫做神经网络（Neural Network），就是类神经网路，这个东西就是Deep Learning！**</span>

<center><img src="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/powerful-network.png" width="60%;"></center>
    </div>

    
    
    

    <div>
      
        <div>
    
        <div style="text-align:center;color: #ccc;font-size:14px;">-------------本文结束<i class="fa fa-paw"></i>感谢您的阅读-------------</div>
    
</div>
      
    </div>
        <div class="reward-container">
  <div>您的支持是对我最大的鼓励！</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="YANG CHEN 微信支付">
        <p>微信支付</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/alipay.jpg" alt="YANG CHEN 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>YANG CHEN
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://yangchen.pro/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/7-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88Logistic-Regression%EF%BC%89/" title="7-逻辑回归（Logistic Regression）">https://yangchen.pro/机器学习/“有手就行”系列/7-逻辑回归（Logistic-Regression）/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"># 机器学习</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/%E2%80%9C%E6%9C%89%E6%89%8B%E5%B0%B1%E8%A1%8C%E2%80%9D%E7%B3%BB%E5%88%97/6-%E5%88%86%E7%B1%BB%EF%BC%88Classification%EF%BC%89/" rel="prev" title="6-分类（Classification）">
      <i class="fa fa-chevron-left"></i> 6-分类（Classification）
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%9B%9E%E9%A1%BE"><span class="nav-text">1 回顾</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="nav-text">2 逻辑回归</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#2-1-Step-1%EF%BC%9Afunction-set"><span class="nav-text">2.1 Step 1：function set</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-2-Step-2%EF%BC%9AGoodness-of-a-function"><span class="nav-text">2.2 Step 2：Goodness of a function</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2-3-step-3%EF%BC%9AFind-the-best-function"><span class="nav-text">2.3 step 3：Find the best function</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92-vs-%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">3 逻辑回归 vs 线性回归</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#3-1-%E6%A8%A1%E5%9E%8B"><span class="nav-text">3.1 模型</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-2-%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="nav-text">3.2 损失函数</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#3-3-%E5%8F%82%E6%95%B0%E6%9B%B4%E6%96%B0"><span class="nav-text">3.3 参数更新</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#7-2-%E5%A4%9A%E5%88%86%E7%B1%BB%E8%BF%87%E7%A8%8B"><span class="nav-text">7.2 多分类过程</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="YANG CHEN"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">YANG CHEN</p>
  <div class="site-description" itemprop="description">is me</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">文章</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/ycv587" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;ycv587" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:not1st@163.com" title="E-Mail → mailto:not1st@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>
  <div class="cc-license motion-element" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="/images/cc-by-nc-sa.svg" alt="Creative Commons"></a>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        


  <script src='https://unpkg.com/mermaid@7.1.2/dist/mermaid.min.js''></script>
  <script>
    if (window.mermaid) {
      mermaid.initialize({theme: 'forest'});
    }
  </script>


<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">YANG CHEN</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="站点总字数">68k字</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:02</span>
</div>

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="总访客量">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="总访问量">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>











<script>
if (document.querySelectorAll('pre.mermaid').length) {
  NexT.utils.getScript('//cdn.jsdelivr.net/npm/mermaid@8/dist/mermaid.min.js', () => {
    mermaid.initialize({
      theme    : '',
      logLevel : 3,
      flowchart: { curve     : 'linear' },
      gantt    : { axisFormat: '%m/%d/%Y' },
      sequence : { actorMargin: 50 }
    });
  }, window.mermaid);
}
</script>


  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    /*
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    */
    window.MathJax = {
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"] ],
        processEscapes: true
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = 'https://cdn.jsdelivr.net/npm/mathjax@2.7.8/unpacked/MathJax.js?config=TeX-MML-AM_CHTML';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
